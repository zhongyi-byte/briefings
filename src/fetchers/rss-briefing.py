#!/usr/bin/env python3
"""
RSS ç®€æŠ¥ç”Ÿæˆå™¨
æŠ“å–ç²¾é€‰ç§äººåšå®¢ RSS
"""

import requests
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

# ç²¾é€‰ RSS åˆ—è¡¨
RSS_FEEDS = [
    {"name": "Simon Willison", "url": "https://simonwillison.net/atom/everything/", 
     "category": "æŠ€æœ¯æ¢ç´¢", "priority": 1, "type": "atom"},
    {"name": "antirez", "url": "http://antirez.com/rss", 
     "category": "ç³»ç»Ÿç¼–ç¨‹", "priority": 1, "type": "rss"},
    {"name": "Rachel by the Bay", "url": "https://rachelbythebay.com/w/atom.xml", 
     "category": "ç³»ç»Ÿæ•…äº‹", "priority": 1, "type": "atom"},
    {"name": "Overreacted", "url": "https://overreacted.io/rss.xml", 
     "category": "ç¼–ç¨‹å“²å­¦", "priority": 1, "type": "rss"},
    {"name": "Dynomight", "url": "https://dynomight.net/feed.xml", 
     "category": "æ·±åº¦æ€è€ƒ", "priority": 1, "type": "rss"},
    {"name": "Sean Goedecke", "url": "https://www.seangoedecke.com/rss.xml", 
     "category": "è½¯ä»¶å·¥ç¨‹", "priority": 2, "type": "rss"},
    {"name": "Mitchell Hashimoto", "url": "https://mitchellh.com/feed.xml", 
     "category": "äº§å“æŠ€æœ¯", "priority": 2, "type": "atom"},
    {"name": "Ken Shirriff", "url": "https://www.righto.com/feeds/posts/default", 
     "category": "ç¡¬ä»¶é€†å‘", "priority": 2, "type": "rss"},
]

def fetch_feed(feed, cutoff):
    """è·å–å•ä¸ª feed"""
    try:
        resp = requests.get(feed['url'], timeout=15)
        
        if feed['type'] == 'atom':
            entries = parse_atom(resp.content)
        else:
            entries = parse_rss(resp.content)
        
        articles = []
        for entry in entries:
            if entry.get('published'):
                try:
                    pub_text = entry['published'].replace('Z', '+00:00')
                    pub_date = datetime.fromisoformat(pub_text)
                    if pub_date >= cutoff:
                        articles.append({
                            **entry,
                            'source': feed['name'],
                            'category': feed['category'],
                            'priority': feed['priority'],
                            'date': pub_date
                        })
                except:
                    pass
        
        return {'feed': feed['name'], 'articles': articles, 'error': None}
    
    except Exception as e:
        return {'feed': feed['name'], 'articles': [], 'error': str(e)[:50]}

def parse_atom(content):
    """è§£æ Atom"""
    root = ET.fromstring(content)
    ns = {'atom': 'http://www.w3.org/2005/Atom'}
    entries = []
    
    for entry in root.findall('.//atom:entry', ns)[:5]:
        title = entry.find('atom:title', ns)
        published = entry.find('atom:published', ns)
        link = entry.find('atom:link', ns)
        
        if title is not None and published is not None:
            entries.append({
                'title': title.text or 'No title',
                'url': link.get('href', '') if link is not None else '',
                'published': published.text
            })
    return entries

def parse_rss(content):
    """è§£æ RSS"""
    root = ET.fromstring(content)
    entries = []
    
    for item in root.findall('.//item')[:5]:
        title = item.find('title')
        pub_date = item.find('pubDate')
        link = item.find('link')
        
        if title is not None:
            entries.append({
                'title': title.text or 'No title',
                'url': link.text if link is not None else '',
                'published': pub_date.text if pub_date is not None else ''
            })
    return entries

def generate_briefing(articles):
    """ç”Ÿæˆç®€æŠ¥"""
    date_str = datetime.now().strftime('%Y-%m-%d')
    
    md = f"""# ğŸ“° RSS ç®€æŠ¥ - {date_str}

> æ¥æº: ç²¾é€‰ç§äººåšå®¢
> æŠ“å–æ—¶é—´: {datetime.now().strftime('%H:%M')}
> æ–°æ–‡ç« : {len(articles)} ç¯‡

---

## ğŸŒŸ æœ€æ–°æ–‡ç« 

"""
    
    priority1 = [a for a in articles if a['priority'] == 1]
    for article in priority1[:15]:
        md += f"""### {article['title']}
- **æ¥æº**: [{article['source']}]({article['url']}) ({article['category']})
- **æ—¥æœŸ**: {article['date'].strftime('%Y-%m-%d %H:%M')}

"""
    
    if len(articles) > len(priority1):
        md += "## ğŸ”§ å…¶ä»–æ–‡ç« \n\n"
        for article in articles[len(priority1):10]:
            md += f"- [{article['title']}]({article['url']}) - {article['source']} ({article['date'].strftime('%m-%d')})\n"
    
    md += f"""
---

*ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}*
*æ¥æº: ç²¾é€‰ç§äººåšå®¢ RSS ({len(RSS_FEEDS)} ä¸ªæº)*
"""
    return md

def main():
    print("=" * 60)
    print("ğŸ¤– RSS ç®€æŠ¥ç”Ÿæˆå™¨ - å¹¶è¡ŒæŠ“å–")
    print("=" * 60)
    
    cutoff = datetime.now(timezone.utc) - timedelta(days=3)
    all_articles = []
    
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {executor.submit(fetch_feed, feed, cutoff): feed for feed in RSS_FEEDS}
        
        for future in as_completed(futures):
            result = future.result()
            if result['error']:
                print(f"ğŸ” {result['feed']}... âŒ {result['error']}")
            else:
                print(f"ğŸ” {result['feed']}... âœ… {len(result['articles'])} ç¯‡")
                all_articles.extend(result['articles'])
    
    all_articles.sort(key=lambda x: x['date'], reverse=True)
    
    print(f"\nğŸ“Š æ‰¾åˆ° {len(all_articles)} ç¯‡æ–°æ–‡ç« ")
    
    briefing = generate_briefing(all_articles)
    
    # ä¿å­˜
    output_dir = Path(__file__).parent.parent / "output"
    output_dir.mkdir(exist_ok=True)
    output_path = output_dir / f"rss-briefing-{datetime.now().strftime('%Y-%m-%d')}.md"
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(briefing)
    
    print(f"ğŸ’¾ ç®€æŠ¥å·²ä¿å­˜: {output_path}")

if __name__ == "__main__":
    main()
