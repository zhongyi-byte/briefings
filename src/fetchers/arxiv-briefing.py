#!/usr/bin/env python3
"""
arXiv AIè®ºæ–‡ç®€æŠ¥ç”Ÿæˆå™¨
æŠ“å–10å®¶é¡¶çº§å‚å•†çš„æœ€æ–°è®ºæ–‡
"""

import urllib.request
import urllib.parse
import socket
import re
from datetime import datetime, timedelta
from pathlib import Path

socket.setdefaulttimeout(15)

# 10å®¶é¡¶çº§å‚å•†
TOP_COMPANIES = [
    ("Google DeepMind", "Google OR DeepMind"),
    ("OpenAI", "OpenAI"),
    ("Anthropic", "Anthropic"),
    ("Meta AI", '"Meta AI" OR "FAIR"'),
    ("Microsoft Research", '"Microsoft Research"'),
    ("NVIDIA", "NVIDIA"),
    ("Stanford", "Stanford"),
    ("UC Berkeley", '"UC Berkeley" OR "Berkeley"'),
    ("MIT", "MIT CSAIL"),
    ("CMU", '"Carnegie Mellon"')
]

def fetch_arxiv_company(company, query):
    """è·å–å•ä¸ªå…¬å¸çš„è®ºæ–‡"""
    papers = []
    yesterday = (datetime.now() - timedelta(days=2)).strftime('%Y%m%d')
    
    try:
        url = f"http://export.arxiv.org/api/query?search_query=au:{urllib.parse.quote(query)}+OR+all:{urllib.parse.quote(query)}&start=0&max_results=3&sortBy=submittedDate&sortOrder=descending"
        req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
        with urllib.request.urlopen(req, timeout=10) as r:
            data = r.read().decode('utf-8')
        
        entries = data.split('<entry>')[1:]
        for entry in entries[:2]:
            try:
                # æå–æ ‡é¢˜
                title_match = re.search(r'<title>(.+?)</title>', entry, re.DOTALL)
                if not title_match:
                    continue
                title = title_match.group(1).strip()
                title = title.replace('\n', ' ').replace('  ', ' ')
                
                # æå–URL
                url_match = re.search(r'<id>(http://arxiv.org/abs/\d+\.\d+)</id>', entry)
                paper_url = url_match.group(1) if url_match else ""
                
                # æå–æ—¥æœŸ
                date_match = re.search(r'<published>(\d{4}-\d{2}-\d{2})', entry)
                pub_date = date_match.group(1) if date_match else ""
                
                # æå–ä½œè€…
                authors = re.findall(r'<name>(.+?)</name>', entry)
                
                if pub_date:
                    papers.append({
                        'title': title,
                        'url': paper_url,
                        'company': company,
                        'date': pub_date,
                        'authors': authors[:3]
                    })
            except:
                continue
        
        return papers
    
    except Exception as e:
        print(f"   {company}: {str(e)[:30]}")
        return []

def fetch_arxiv():
    """è·å–æ‰€æœ‰å…¬å¸è®ºæ–‡"""
    print("ğŸ” è·å– arXiv AIè®ºæ–‡...")
    print(f"   ç›‘æ§ {len(TOP_COMPANIES)} å®¶æœºæ„")
    
    all_papers = []
    for company, query in TOP_COMPANIES:
        papers = fetch_arxiv_company(company, query)
        if papers:
            print(f"   âœ… {company}: {len(papers)} ç¯‡")
            all_papers.extend(papers)
        else:
            print(f"   âš ï¸ {company}: æ— æ–°è®ºæ–‡")
    
    print(f"\n   æ€»è®¡: {len(all_papers)} ç¯‡")
    return all_papers

def generate_briefing(papers):
    """ç”Ÿæˆ arXiv ç®€æŠ¥"""
    date_str = datetime.now().strftime('%Y-%m-%d')
    
    md = f"""# ğŸ“„ arXiv AIè®ºæ–‡ç®€æŠ¥ - {date_str}

> æ¥æº: arXiv (10å®¶é¡¶çº§å‚å•†)
> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%H:%M')}
> å…± {len(papers)} ç¯‡æ–°è®ºæ–‡

---

"""
    
    if not papers:
        md += "ä»Šæ—¥æš‚æ— æ–°è®ºæ–‡\n\n"
    else:
        # æŒ‰å…¬å¸åˆ†ç»„
        by_company = {}
        for p in papers:
            c = p['company']
            if c not in by_company:
                by_company[c] = []
            by_company[c].append(p)
        
        for company, company_papers in sorted(by_company.items()):
            md += f"## {company}\n\n"
            for p in company_papers:
                md += f"- **[{p['title'][:80]}]({p['url']})**\n"
                if p['authors']:
                    md += f"  ğŸ‘¤ {', '.join(p['authors'])}\n"
                md += f"  ğŸ“… {p['date']}\n\n"
    
    md += f"""---

*ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}*
*æ¥æº: [arXiv](https://arxiv.org)*

ç›‘æ§æœºæ„:
"""
    for company, _ in TOP_COMPANIES:
        md += f"- {company}\n"
    
    return md

def main():
    print("=" * 60)
    print("ğŸ“„ arXiv AIè®ºæ–‡ç®€æŠ¥ç”Ÿæˆå™¨")
    print("=" * 60)
    print()
    
    papers = fetch_arxiv()
    briefing = generate_briefing(papers)
    
    # ä¿å­˜
    output_dir = Path(__file__).parent.parent.parent / "output"
    output_dir.mkdir(exist_ok=True)
    output_path = output_dir / f"arxiv-briefing-{datetime.now().strftime('%Y-%m-%d')}.md"
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(briefing)
    
    print(f"\nğŸ’¾ å·²ä¿å­˜: {output_path}")

if __name__ == "__main__":
    main()
